{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models import CoherenceModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ticker</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>content</th>\n",
       "      <th>release_date</th>\n",
       "      <th>provider</th>\n",
       "      <th>url</th>\n",
       "      <th>article_id</th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>clean_content</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_content</th>\n",
       "      <th>new_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>221539</td>\n",
       "      <td>NIO</td>\n",
       "      <td>A Central Bank War Just Started And Its Good F...</td>\n",
       "      <td>opinion</td>\n",
       "      <td>ECB Effects\\nThe move in the euro was huge  fa...</td>\n",
       "      <td>2019-03-07</td>\n",
       "      <td>Michael Kramer</td>\n",
       "      <td>https://www.investing.com/analysis/a-central-b...</td>\n",
       "      <td>200395687</td>\n",
       "      <td>2019-03-07</td>\n",
       "      <td>3.77750</td>\n",
       "      <td>3.73150</td>\n",
       "      <td>ecb effect move euro huge falling pip huge imp...</td>\n",
       "      <td>0</td>\n",
       "      <td>['ecb', 'effect', 'move', 'euro', 'huge', 'fal...</td>\n",
       "      <td>[ecb, effect, move, euro, huge, falling, pip, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>221547</td>\n",
       "      <td>NIO</td>\n",
       "      <td>6 Stocks To Watch  Nivida Could Be Falling</td>\n",
       "      <td>opinion</td>\n",
       "      <td>6 Stocks To Watch  March 6 Trading Session\\nSt...</td>\n",
       "      <td>2019-03-06</td>\n",
       "      <td>Michael Kramer</td>\n",
       "      <td>https://www.investing.com/analysis/6-stocks-to...</td>\n",
       "      <td>200394931</td>\n",
       "      <td>2019-03-06</td>\n",
       "      <td>3.90400</td>\n",
       "      <td>3.80125</td>\n",
       "      <td>stock watch march trading session stock stock ...</td>\n",
       "      <td>0</td>\n",
       "      <td>['stock', 'watch', 'march', 'trading', 'sessio...</td>\n",
       "      <td>[stock, watch, march, trading, session, stock,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>221572</td>\n",
       "      <td>NIO</td>\n",
       "      <td>Stocks   Dow Drops Nearly 400 Points as Apple ...</td>\n",
       "      <td>news</td>\n",
       "      <td>Investing com   A rout in Apple and Facebook  ...</td>\n",
       "      <td>2018-11-19</td>\n",
       "      <td>Investing.com</td>\n",
       "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
       "      <td>1694042</td>\n",
       "      <td>2018-11-19</td>\n",
       "      <td>4.04475</td>\n",
       "      <td>3.61750</td>\n",
       "      <td>investing com rout apple facebook nasdaq fb mo...</td>\n",
       "      <td>0</td>\n",
       "      <td>['investing', 'com', 'rout', 'apple', 'faceboo...</td>\n",
       "      <td>[investing, com, rout, apple, facebook, nasdaq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>221597</td>\n",
       "      <td>UBER</td>\n",
       "      <td>The Best Of CES 2020  Revised</td>\n",
       "      <td>opinion</td>\n",
       "      <td>With 4 500 companies bringing their innovation...</td>\n",
       "      <td>2020-01-16</td>\n",
       "      <td>Zacks Investment Research</td>\n",
       "      <td>https://www.investing.com/analysis/the-best-of...</td>\n",
       "      <td>200499164</td>\n",
       "      <td>2020-01-16</td>\n",
       "      <td>6.19300</td>\n",
       "      <td>6.22325</td>\n",
       "      <td>company bringing innovation ce jan get really ...</td>\n",
       "      <td>1</td>\n",
       "      <td>['company', 'bringing', 'innovation', 'ce', 'j...</td>\n",
       "      <td>[company, bringing, innovation, ce, jan, get, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>221614</td>\n",
       "      <td>UBER</td>\n",
       "      <td>The Best Of CES 2020</td>\n",
       "      <td>opinion</td>\n",
       "      <td>With 4 500 companies bringing their innovation...</td>\n",
       "      <td>2020-01-10</td>\n",
       "      <td>Zacks Investment Research</td>\n",
       "      <td>https://www.investing.com/analysis/the-best-of...</td>\n",
       "      <td>200498063</td>\n",
       "      <td>2020-01-10</td>\n",
       "      <td>6.18325</td>\n",
       "      <td>6.10800</td>\n",
       "      <td>company bringing innovation ce jan get really ...</td>\n",
       "      <td>0</td>\n",
       "      <td>['company', 'bringing', 'innovation', 'ce', 'j...</td>\n",
       "      <td>[company, bringing, innovation, ce, jan, get, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id ticker                                              title category  \\\n",
       "0  221539    NIO  A Central Bank War Just Started And Its Good F...  opinion   \n",
       "1  221547    NIO         6 Stocks To Watch  Nivida Could Be Falling  opinion   \n",
       "2  221572    NIO  Stocks   Dow Drops Nearly 400 Points as Apple ...     news   \n",
       "3  221597   UBER                     The Best Of CES 2020  Revised   opinion   \n",
       "4  221614   UBER                               The Best Of CES 2020  opinion   \n",
       "\n",
       "                                             content release_date  \\\n",
       "0  ECB Effects\\nThe move in the euro was huge  fa...   2019-03-07   \n",
       "1  6 Stocks To Watch  March 6 Trading Session\\nSt...   2019-03-06   \n",
       "2  Investing com   A rout in Apple and Facebook  ...   2018-11-19   \n",
       "3  With 4 500 companies bringing their innovation...   2020-01-16   \n",
       "4  With 4 500 companies bringing their innovation...   2020-01-10   \n",
       "\n",
       "                    provider  \\\n",
       "0             Michael Kramer   \n",
       "1             Michael Kramer   \n",
       "2              Investing.com   \n",
       "3  Zacks Investment Research   \n",
       "4  Zacks Investment Research   \n",
       "\n",
       "                                                 url  article_id        Date  \\\n",
       "0  https://www.investing.com/analysis/a-central-b...   200395687  2019-03-07   \n",
       "1  https://www.investing.com/analysis/6-stocks-to...   200394931  2019-03-06   \n",
       "2  https://www.investing.com/news/stock-market-ne...     1694042  2018-11-19   \n",
       "3  https://www.investing.com/analysis/the-best-of...   200499164  2020-01-16   \n",
       "4  https://www.investing.com/analysis/the-best-of...   200498063  2020-01-10   \n",
       "\n",
       "      Open    Close                                      clean_content  label  \\\n",
       "0  3.77750  3.73150  ecb effect move euro huge falling pip huge imp...      0   \n",
       "1  3.90400  3.80125  stock watch march trading session stock stock ...      0   \n",
       "2  4.04475  3.61750  investing com rout apple facebook nasdaq fb mo...      0   \n",
       "3  6.19300  6.22325  company bringing innovation ce jan get really ...      1   \n",
       "4  6.18325  6.10800  company bringing innovation ce jan get really ...      0   \n",
       "\n",
       "                                   tokenized_content  \\\n",
       "0  ['ecb', 'effect', 'move', 'euro', 'huge', 'fal...   \n",
       "1  ['stock', 'watch', 'march', 'trading', 'sessio...   \n",
       "2  ['investing', 'com', 'rout', 'apple', 'faceboo...   \n",
       "3  ['company', 'bringing', 'innovation', 'ce', 'j...   \n",
       "4  ['company', 'bringing', 'innovation', 'ce', 'j...   \n",
       "\n",
       "                                       new_tokenized  \n",
       "0  [ecb, effect, move, euro, huge, falling, pip, ...  \n",
       "1  [stock, watch, march, trading, session, stock,...  \n",
       "2  [investing, com, rout, apple, facebook, nasdaq...  \n",
       "3  [company, bringing, innovation, ce, jan, get, ...  \n",
       "4  [company, bringing, innovation, ce, jan, get, ...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the data\n",
    "path = 'C:/Users/20202201/Documents/Uni/Master/Year2/Semester 1/Natural Language Processing/NLP/Assignment-1/Dataset/assignment-2-data.csv'\n",
    "df_cleaned = pd.read_csv(path)\n",
    "\n",
    "df_cleaned['new_tokenized'] = df_cleaned['clean_content'].apply(lambda x: x.split())\n",
    "\n",
    "df_cleaned.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary\n",
    "dictionary = corpora.Dictionary(df_cleaned['new_tokenized'])\n",
    "\n",
    "# Convert the list of documents (corpus) into Document Term Matrix using the dictionary prepared above\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in df_cleaned['new_tokenized']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train, validation and test\n",
    "train = doc_term_matrix[:int(len(doc_term_matrix)*0.8)]\n",
    "validation = doc_term_matrix[int(len(doc_term_matrix)*0.8):int(len(doc_term_matrix)*0.9)]\n",
    "test = doc_term_matrix[int(len(doc_term_matrix)*0.9):]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LDA model with num_topics=2, alpha=symmetric, eta=symmetric, chunksize=100, passes=10, iterations=50\n"
     ]
    }
   ],
   "source": [
    "# Define the best coherence score\n",
    "best_coherence_score = 0\n",
    "\n",
    "# Define the best parameters\n",
    "best_params = {}\n",
    "\n",
    "# Iterate over the parameters\n",
    "for num_topics in grid['num_topics']:\n",
    "    for alpha in grid['alpha']:\n",
    "        for eta in grid['eta']:\n",
    "            for chunksize in grid['chunksize']:\n",
    "                for passes in grid['passes']:\n",
    "                    for iterations in grid['iterations']:\n",
    "                        print(f\"Training LDA model with num_topics={num_topics}, alpha={alpha}, eta={eta}, chunksize={chunksize}, passes={passes}, iterations={iterations}\")\n",
    "                        # Create the LDA model\n",
    "                        lda_model = LdaModel(corpus=train, id2word=dictionary, num_topics=num_topics, alpha=alpha, eta=eta, chunksize=chunksize, passes=passes, iterations=iterations)\n",
    "                        coherence_model = CoherenceModel(model=lda_model, texts=df_cleaned['new_tokenized'][:int(len(df_cleaned)*0.8)], dictionary=dictionary, coherence='c_v')\n",
    "                        coherence_score = coherence_model.get_coherence()\n",
    "                        print(f\"Coherence Score: {coherence_score}\")\n",
    "                        if coherence_score > best_coherence_score:\n",
    "                            best_coherence_score = coherence_score\n",
    "                            best_params['num_topics'] = num_topics\n",
    "                            best_params['alpha'] = alpha\n",
    "                            best_params['eta'] = eta\n",
    "                            best_params['chunksize'] = chunksize\n",
    "                            best_params['passes'] = passes\n",
    "                            best_params['iterations'] = iterations\n",
    "                            best_params['coherence_score'] = coherence_score\n",
    "                            print(f\"New best coherence score: {best_coherence_score} with params: {best_params}\")\n",
    "\n",
    "# Train the best model on the combined train and validation data\n",
    "best_lda_model = LdaModel(corpus=train + validation, id2word=dictionary, num_topics=best_params['num_topics'], alpha=best_params['alpha'], eta=best_params['eta'], chunksize=best_params['chunksize'], passes=best_params['passes'], iterations=best_params['iterations'])\n",
    "\n",
    "# Evaluate the best model on the test data\n",
    "coherence_model_test = CoherenceModel(model=best_lda_model, texts=df_cleaned['new_tokenized'][int(len(df_cleaned)*0.9):], dictionary=dictionary, coherence='c_v')\n",
    "test_coherence_score = coherence_model_test.get_coherence()\n",
    "print(f\"Test Coherence Score: {test_coherence_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.012*\"stock\" + 0.008*\"market\" + 0.008*\"year\" + 0.006*\"company\" + 0.005*\"earnings\" + 0.005*\"week\" + 0.005*\"nasdaq\" + 0.005*\"nvidia\" + 0.005*\"share\" + 0.005*\"price\"')\n",
      "(1, '0.013*\"year\" + 0.013*\"company\" + 0.013*\"stock\" + 0.012*\"zacks\" + 0.010*\"nasdaq\" + 0.010*\"earnings\" + 0.008*\"nvidia\" + 0.008*\"share\" + 0.008*\"quarter\" + 0.008*\"market\"')\n",
      "(2, '0.013*\"year\" + 0.012*\"nasdaq\" + 0.011*\"company\" + 0.010*\"quarter\" + 0.007*\"zacks\" + 0.007*\"stock\" + 0.007*\"nvidia\" + 0.006*\"growth\" + 0.006*\"million\" + 0.006*\"inc\"')\n",
      "0.37455136700082603\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Print the topics\n",
    "for topic in lda_model.print_topics():\n",
    " print(topic)\n",
    "#Get coherence score\n",
    "print(CoherenceModel(model=lda_model, texts=df_cleaned['new_tokenized'], dictionary=dictionary, coherence='c_v').get_coherence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JADS_NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
