{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fuzzytm in /opt/homebrew/anaconda3/envs/JADS-NLP/lib/python3.9/site-packages (2.0.9)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/anaconda3/envs/JADS-NLP/lib/python3.9/site-packages (from fuzzytm) (1.24.4)\n",
      "Requirement already satisfied: pandas in /opt/homebrew/anaconda3/envs/JADS-NLP/lib/python3.9/site-packages (from fuzzytm) (1.5.3)\n",
      "Collecting scipy (from fuzzytm)\n",
      "  Using cached scipy-1.13.1-cp39-cp39-macosx_12_0_arm64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: pyfume in /opt/homebrew/anaconda3/envs/JADS-NLP/lib/python3.9/site-packages (from fuzzytm) (0.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/homebrew/anaconda3/envs/JADS-NLP/lib/python3.9/site-packages (from pandas->fuzzytm) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/anaconda3/envs/JADS-NLP/lib/python3.9/site-packages (from pandas->fuzzytm) (2024.2)\n",
      "  Using cached scipy-1.10.1-cp39-cp39-macosx_12_0_arm64.whl.metadata (53 kB)\n",
      "Requirement already satisfied: simpful==2.12.0 in /opt/homebrew/anaconda3/envs/JADS-NLP/lib/python3.9/site-packages (from pyfume->fuzzytm) (2.12.0)\n",
      "Requirement already satisfied: fst-pso==1.8.1 in /opt/homebrew/anaconda3/envs/JADS-NLP/lib/python3.9/site-packages (from pyfume->fuzzytm) (1.8.1)\n",
      "Requirement already satisfied: miniful in /opt/homebrew/anaconda3/envs/JADS-NLP/lib/python3.9/site-packages (from fst-pso==1.8.1->pyfume->fuzzytm) (0.0.6)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/anaconda3/envs/JADS-NLP/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->fuzzytm) (1.16.0)\n",
      "Using cached scipy-1.10.1-cp39-cp39-macosx_12_0_arm64.whl (28.9 MB)\n",
      "Installing collected packages: scipy\n",
      "Successfully installed scipy-1.10.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: gensim in /opt/homebrew/anaconda3/envs/JADS-NLP/lib/python3.9/site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /opt/homebrew/anaconda3/envs/JADS-NLP/lib/python3.9/site-packages (from gensim) (1.24.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /opt/homebrew/anaconda3/envs/JADS-NLP/lib/python3.9/site-packages (from gensim) (1.10.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/homebrew/anaconda3/envs/JADS-NLP/lib/python3.9/site-packages (from gensim) (7.0.5)\n",
      "Requirement already satisfied: wrapt in /opt/homebrew/anaconda3/envs/JADS-NLP/lib/python3.9/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install fuzzytm\n",
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from FuzzyTM import FLSA_W, FLSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ticker</th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>content</th>\n",
       "      <th>release_date</th>\n",
       "      <th>provider</th>\n",
       "      <th>url</th>\n",
       "      <th>article_id</th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>clean_content</th>\n",
       "      <th>label</th>\n",
       "      <th>tokenized_content</th>\n",
       "      <th>new_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>221539</td>\n",
       "      <td>NIO</td>\n",
       "      <td>A Central Bank War Just Started And Its Good F...</td>\n",
       "      <td>opinion</td>\n",
       "      <td>ECB Effects\\nThe move in the euro was huge  fa...</td>\n",
       "      <td>2019-03-07</td>\n",
       "      <td>Michael Kramer</td>\n",
       "      <td>https://www.investing.com/analysis/a-central-b...</td>\n",
       "      <td>200395687</td>\n",
       "      <td>2019-03-07</td>\n",
       "      <td>3.77750</td>\n",
       "      <td>3.73150</td>\n",
       "      <td>ecb effect move euro huge falling pip huge imp...</td>\n",
       "      <td>0</td>\n",
       "      <td>['ecb', 'effect', 'move', 'euro', 'huge', 'fal...</td>\n",
       "      <td>[ecb, effect, move, euro, huge, falling, pip, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>221547</td>\n",
       "      <td>NIO</td>\n",
       "      <td>6 Stocks To Watch  Nivida Could Be Falling</td>\n",
       "      <td>opinion</td>\n",
       "      <td>6 Stocks To Watch  March 6 Trading Session\\nSt...</td>\n",
       "      <td>2019-03-06</td>\n",
       "      <td>Michael Kramer</td>\n",
       "      <td>https://www.investing.com/analysis/6-stocks-to...</td>\n",
       "      <td>200394931</td>\n",
       "      <td>2019-03-06</td>\n",
       "      <td>3.90400</td>\n",
       "      <td>3.80125</td>\n",
       "      <td>stock watch march trading session stock stock ...</td>\n",
       "      <td>0</td>\n",
       "      <td>['stock', 'watch', 'march', 'trading', 'sessio...</td>\n",
       "      <td>[stock, watch, march, trading, session, stock,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>221572</td>\n",
       "      <td>NIO</td>\n",
       "      <td>Stocks   Dow Drops Nearly 400 Points as Apple ...</td>\n",
       "      <td>news</td>\n",
       "      <td>Investing com   A rout in Apple and Facebook  ...</td>\n",
       "      <td>2018-11-19</td>\n",
       "      <td>Investing.com</td>\n",
       "      <td>https://www.investing.com/news/stock-market-ne...</td>\n",
       "      <td>1694042</td>\n",
       "      <td>2018-11-19</td>\n",
       "      <td>4.04475</td>\n",
       "      <td>3.61750</td>\n",
       "      <td>investing com rout apple facebook nasdaq fb mo...</td>\n",
       "      <td>0</td>\n",
       "      <td>['investing', 'com', 'rout', 'apple', 'faceboo...</td>\n",
       "      <td>[investing, com, rout, apple, facebook, nasdaq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>221597</td>\n",
       "      <td>UBER</td>\n",
       "      <td>The Best Of CES 2020  Revised</td>\n",
       "      <td>opinion</td>\n",
       "      <td>With 4 500 companies bringing their innovation...</td>\n",
       "      <td>2020-01-16</td>\n",
       "      <td>Zacks Investment Research</td>\n",
       "      <td>https://www.investing.com/analysis/the-best-of...</td>\n",
       "      <td>200499164</td>\n",
       "      <td>2020-01-16</td>\n",
       "      <td>6.19300</td>\n",
       "      <td>6.22325</td>\n",
       "      <td>company bringing innovation ce jan get really ...</td>\n",
       "      <td>1</td>\n",
       "      <td>['company', 'bringing', 'innovation', 'ce', 'j...</td>\n",
       "      <td>[company, bringing, innovation, ce, jan, get, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>221614</td>\n",
       "      <td>UBER</td>\n",
       "      <td>The Best Of CES 2020</td>\n",
       "      <td>opinion</td>\n",
       "      <td>With 4 500 companies bringing their innovation...</td>\n",
       "      <td>2020-01-10</td>\n",
       "      <td>Zacks Investment Research</td>\n",
       "      <td>https://www.investing.com/analysis/the-best-of...</td>\n",
       "      <td>200498063</td>\n",
       "      <td>2020-01-10</td>\n",
       "      <td>6.18325</td>\n",
       "      <td>6.10800</td>\n",
       "      <td>company bringing innovation ce jan get really ...</td>\n",
       "      <td>0</td>\n",
       "      <td>['company', 'bringing', 'innovation', 'ce', 'j...</td>\n",
       "      <td>[company, bringing, innovation, ce, jan, get, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id ticker                                              title category  \\\n",
       "0  221539    NIO  A Central Bank War Just Started And Its Good F...  opinion   \n",
       "1  221547    NIO         6 Stocks To Watch  Nivida Could Be Falling  opinion   \n",
       "2  221572    NIO  Stocks   Dow Drops Nearly 400 Points as Apple ...     news   \n",
       "3  221597   UBER                     The Best Of CES 2020  Revised   opinion   \n",
       "4  221614   UBER                               The Best Of CES 2020  opinion   \n",
       "\n",
       "                                             content release_date  \\\n",
       "0  ECB Effects\\nThe move in the euro was huge  fa...   2019-03-07   \n",
       "1  6 Stocks To Watch  March 6 Trading Session\\nSt...   2019-03-06   \n",
       "2  Investing com   A rout in Apple and Facebook  ...   2018-11-19   \n",
       "3  With 4 500 companies bringing their innovation...   2020-01-16   \n",
       "4  With 4 500 companies bringing their innovation...   2020-01-10   \n",
       "\n",
       "                    provider  \\\n",
       "0             Michael Kramer   \n",
       "1             Michael Kramer   \n",
       "2              Investing.com   \n",
       "3  Zacks Investment Research   \n",
       "4  Zacks Investment Research   \n",
       "\n",
       "                                                 url  article_id        Date  \\\n",
       "0  https://www.investing.com/analysis/a-central-b...   200395687  2019-03-07   \n",
       "1  https://www.investing.com/analysis/6-stocks-to...   200394931  2019-03-06   \n",
       "2  https://www.investing.com/news/stock-market-ne...     1694042  2018-11-19   \n",
       "3  https://www.investing.com/analysis/the-best-of...   200499164  2020-01-16   \n",
       "4  https://www.investing.com/analysis/the-best-of...   200498063  2020-01-10   \n",
       "\n",
       "      Open    Close                                      clean_content  label  \\\n",
       "0  3.77750  3.73150  ecb effect move euro huge falling pip huge imp...      0   \n",
       "1  3.90400  3.80125  stock watch march trading session stock stock ...      0   \n",
       "2  4.04475  3.61750  investing com rout apple facebook nasdaq fb mo...      0   \n",
       "3  6.19300  6.22325  company bringing innovation ce jan get really ...      1   \n",
       "4  6.18325  6.10800  company bringing innovation ce jan get really ...      0   \n",
       "\n",
       "                                   tokenized_content  \\\n",
       "0  ['ecb', 'effect', 'move', 'euro', 'huge', 'fal...   \n",
       "1  ['stock', 'watch', 'march', 'trading', 'sessio...   \n",
       "2  ['investing', 'com', 'rout', 'apple', 'faceboo...   \n",
       "3  ['company', 'bringing', 'innovation', 'ce', 'j...   \n",
       "4  ['company', 'bringing', 'innovation', 'ce', 'j...   \n",
       "\n",
       "                                       new_tokenized  \n",
       "0  [ecb, effect, move, euro, huge, falling, pip, ...  \n",
       "1  [stock, watch, march, trading, session, stock,...  \n",
       "2  [investing, com, rout, apple, facebook, nasdaq...  \n",
       "3  [company, bringing, innovation, ce, jan, get, ...  \n",
       "4  [company, bringing, innovation, ce, jan, get, ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the data\n",
    "path = '../Assignment-1/Dataset/assignment-2-data.csv'\n",
    "df_cleaned = pd.read_csv(path)\n",
    "\n",
    "df_cleaned['new_tokenized'] = df_cleaned['clean_content'].apply(lambda x: x.split())\n",
    "\n",
    "df_cleaned.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = df_cleaned['new_tokenized'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29908\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "type matrix doesn't define __round__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(flsaW\u001b[38;5;241m.\u001b[39mget_vocabulary_size())\n\u001b[1;32m      5\u001b[0m pwgt,ptgd \u001b[38;5;241m=\u001b[39m flsaW\u001b[38;5;241m.\u001b[39mget_matrices()\n\u001b[0;32m----> 7\u001b[0m \u001b[43mflsaW\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m topic \u001b[38;5;129;01min\u001b[39;00m flsaW\u001b[38;5;241m.\u001b[39mshow_topics(representation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwords\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     10\u001b[0m   \u001b[38;5;28mprint\u001b[39m(topic)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/JADS-NLP/lib/python3.9/site-packages/FuzzyTM/FuzzyTM.py:842\u001b[0m, in \u001b[0;36mFuzzyTM.show_topics\u001b[0;34m(self, prob_word_given_topic, num_words, index_to_word, representation)\u001b[0m\n\u001b[1;32m    840\u001b[0m     sorted_highest_weight_indices \u001b[38;5;241m=\u001b[39m prob_word_given_topic[:,topic_index]\u001b[38;5;241m.\u001b[39margsort()[\u001b[38;5;241m-\u001b[39mnum_words:][::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    841\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word_index \u001b[38;5;129;01min\u001b[39;00m sorted_highest_weight_indices:\n\u001b[0;32m--> 842\u001b[0m         weight_words \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mround\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprob_word_given_topic\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtopic_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m) \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    843\u001b[0m                          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m index_to_word[word_index] \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m + \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    844\u001b[0m     topic_list\u001b[38;5;241m.\u001b[39mappend((topic_index, weight_words[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m]))\n\u001b[1;32m    845\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m topic_list\n",
      "\u001b[0;31mTypeError\u001b[0m: type matrix doesn't define __round__ method"
     ]
    }
   ],
   "source": [
    "flsaW = FLSA(input_file=input_file, num_topics=10, num_words=10)\n",
    "\n",
    "print(flsaW.get_vocabulary_size())\n",
    "\n",
    "pwgt,ptgd = flsaW.get_matrices()\n",
    "\n",
    "flsaW.show_topics()\n",
    "\n",
    "for topic in flsaW.show_topics(representation='words'):\n",
    "  print(topic)\n",
    "\n",
    "print(flsaW.get_coherence_score())\n",
    "print(flsaW.get_diversity_score())\n",
    "print(flsaW.get_interpretability_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train, validation and test\n",
    "train = input_file[:int(len(input_file)*0.8)]\n",
    "test = input_file[int(len(input_file)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training FLSA model with num_topics=5, num_words=5\n",
      "Vocabulary size: 28219\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'matrix' object has no attribute 'multiply'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m flsaW \u001b[38;5;241m=\u001b[39m FLSA_W(input_file\u001b[38;5;241m=\u001b[39mtrain, num_topics\u001b[38;5;241m=\u001b[39mnum_topics, num_words\u001b[38;5;241m=\u001b[39mnum_words)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVocabulary size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mflsaW\u001b[38;5;241m.\u001b[39mget_vocabulary_size()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m pwgt, ptgd \u001b[38;5;241m=\u001b[39m \u001b[43mflsaW\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_matrices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTopics: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mflsaW\u001b[38;5;241m.\u001b[39mshow_topics()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m coherence_score \u001b[38;5;241m=\u001b[39m flsaW\u001b[38;5;241m.\u001b[39mget_coherence_score()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/JADS-NaturalLanguageProcessing/lib/python3.11/site-packages/FuzzyTM/FuzzyTM.py:1421\u001b[0m, in \u001b[0;36mFLSA_W.get_matrices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1411\u001b[0m projected_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_projected_data(\n\u001b[1;32m   1412\u001b[0m     algorithm \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflsa-w\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1413\u001b[0m     sparse_weighted_matrix \u001b[38;5;241m=\u001b[39m sparse_global_term_weighting,\n\u001b[1;32m   1414\u001b[0m     svd_factors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msvd_factors,\n\u001b[1;32m   1415\u001b[0m     )\n\u001b[1;32m   1416\u001b[0m partition_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_partition_matrix(\n\u001b[1;32m   1417\u001b[0m     data \u001b[38;5;241m=\u001b[39m projected_data,\n\u001b[1;32m   1418\u001b[0m     number_of_clusters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_topics,\n\u001b[1;32m   1419\u001b[0m     method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcluster_method,\n\u001b[1;32m   1420\u001b[0m     )\n\u001b[0;32m-> 1421\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_probability_matrices\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1422\u001b[0m \u001b[43m    \u001b[49m\u001b[43malgorithm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mflsa-w\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprob_topic_given_word_transpose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpartition_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mglobal_term_weights\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msparse_global_term_weighting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1425\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/JADS-NaturalLanguageProcessing/lib/python3.11/site-packages/FuzzyTM/FuzzyTM.py:775\u001b[0m, in \u001b[0;36mFuzzyTM._create_probability_matrices\u001b[0;34m(self, algorithm, prob_topic_given_document_transpose, prob_topic_given_word_transpose, local_term_weights, global_term_weights)\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m algorithm \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m    771\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflsa-v\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    772\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflsa-e\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    773\u001b[0m         ]:\n\u001b[1;32m    774\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prob_word_given_document \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(local_term_weights \u001b[38;5;241m/\u001b[39m local_term_weights\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m--> 775\u001b[0m prob_document_given_word \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prob_word_given_document\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiply\u001b[49m(np\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prob_document_j, (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)))\u001b[38;5;241m.\u001b[39mtoarray() \u001b[38;5;241m/\u001b[39mnp\u001b[38;5;241m.\u001b[39mreshape(np\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prob_word_i), (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m    776\u001b[0m prob_document_given_topic \u001b[38;5;241m=\u001b[39m prob_document_given_word\u001b[38;5;241m.\u001b[39mdot(\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prob_word_given_topic\n\u001b[1;32m    778\u001b[0m     )\n\u001b[1;32m    779\u001b[0m prob_topic_given_document \u001b[38;5;241m=\u001b[39m ((prob_document_given_topic \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prob_topic_k)\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m/\u001b[39m\n\u001b[1;32m    780\u001b[0m                                    \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prob_document_j)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'matrix' object has no attribute 'multiply'"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "best_params = {}\n",
    "\n",
    "grid = {\n",
    "    'num_topics': [5, 10, 30, 50, 75, 100],\n",
    "    'num_words': [5, 10, 30, 50, 75, 100]\n",
    "}\n",
    "\n",
    "# Iterate over the parameters\n",
    "for num_topics in grid['num_topics']:\n",
    "    for num_words in grid['num_words']:\n",
    "        print(f\"Training FLSA model with num_topics={num_topics}, num_words={num_words}\")\n",
    "        \n",
    "        flsaW = FLSA_W(input_file=train, num_topics=num_topics, num_words=num_words)\n",
    "        \n",
    "        print(f\"Vocabulary size: {flsaW.get_vocabulary_size()}\")\n",
    "        \n",
    "        pwgt, ptgd = flsaW.get_matrices()\n",
    "        \n",
    "        print(f\"Topics: {flsaW.show_topics()}\")\n",
    "        \n",
    "        coherence_score = flsaW.get_coherence_score()\n",
    "        diversity_score = flsaW.get_diversity_score()\n",
    "        interpretability_score = flsaW.get_interpretability_score()\n",
    "        \n",
    "        print(f\"Coherence Score: {coherence_score}\")\n",
    "        print(f\"Diversity Score: {diversity_score}\")\n",
    "        print(f\"Interpretability Score: {interpretability_score}\")\n",
    "        \n",
    "        if (coherence_score + diversity_score + interpretability_score > best_score):\n",
    "            best_params = {'num_topics': num_topics, 'num_words': num_words}\n",
    "            best_score = coherence_score + diversity_score + interpretability_score\n",
    "        \n",
    "# Train the best model on the combined train and validation data\n",
    "test_flsa_model = FLSA_W(input_file=test, num_topics=best_params['num_topics'], num_words=best_params['num_words'])\n",
    "\n",
    "test_coherence_score = test_flsa_model.get_coherence_score()\n",
    "test_diversity_score = test_flsa_model.get_diversity_score()\n",
    "test_interpretability_score = test_flsa_model.get_interpretability_score()\n",
    "\n",
    "\n",
    "print(f\"Test Coherence Score: {test_coherence_score}\")\n",
    "print(f\"Test Diversity Score: {test_diversity_score}\")\n",
    "print(f\"Test Interpretability Score: {test_interpretability_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JADS-NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
